{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\nfrom datasets import load_dataset\n\n# Load the FeTaQA dataset\ndataset = load_dataset(\"DongfuJiang/FeTaQA\")\n\n# Access the train, validation, and test splits\ntrain_data = dataset[\"train\"]\nval_data = dataset[\"validation\"]\ntest_data = dataset[\"test\"]\n\n# Display the first example in the training set\nprint(train_data[0])\n# Function to flatten a table \ndef flatten_table(table):\n    flattened = []\n    headers = table[0]  # First row is the header\n    for row in table[1:]:\n        for col_header, col_value in zip(headers, row):\n            flattened.append((col_header, col_value))\n    return flattened","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:21:27.920373Z","iopub.execute_input":"2025-05-09T10:21:27.920693Z","iopub.status.idle":"2025-05-09T10:21:31.688151Z","shell.execute_reply.started":"2025-05-09T10:21:27.920673Z","shell.execute_reply":"2025-05-09T10:21:31.687357Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n{'feta_id': 18162, 'table_source_json': 'totto_source/train_json/example-10461.json', 'page_wikipedia_url': 'http://en.wikipedia.org/wiki/1982_Illinois_gubernatorial_election', 'table_page_title': '1982 Illinois gubernatorial election', 'table_section_title': 'Results', 'table_array': [['Party', 'Party', 'Candidate', 'Votes', '%', 'Â±'], ['-', 'Republican', 'James R. Thompson (incumbent)', '1,816,101', '49.44', '-'], ['-', 'Democratic', 'Adlai Stevenson III', '1,811,027', '49.30', '-'], ['-', 'Libertarian', 'Bea Armstrong', '24,417', '0.66', '-'], ['-', 'Taxpayers', 'John E. Roche', '22,001', '0.60', '-'], ['-', 'N/A', 'write-ins', '161', '0.00', 'n-a'], ['Majority', 'Majority', 'Majority', '5,074', '0.14', '-'], ['Turnout', 'Turnout', 'Turnout', '3,673,707', '-', '-'], ['-', 'Republican hold', 'Republican hold', 'Swing', '-', '-']], 'highlighted_cell_ids': [[1, 2], [6, 3]], 'question': 'Who won the 1982 Illinois gubernatorial election, and how many votes was the margin?', 'answer': 'Thompson prevailed in the 1982 Illinois gubernatorial election by a 5,074 vote margin.'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def prepare_data_for_model(dataset, mode='tabular'):\n    prepared_data = []\n\n    for example in dataset:\n        question = example['question']\n        answer = example['answer']\n\n        if mode == 'tabular':\n            table = example['table_array']\n            flattened_table = flatten_table(table)\n            input_text = f\"Question: {question} Context: {example['table_page_title']} Table: {flattened_table}\"\n            adapter = \"tabular_adapter\"\n\n        elif mode == 'textual':\n            context = example.get(\"context\", \"No context available\")\n            input_text = f\"Question: {question} Context: {context}\"\n            adapter = \"textual_adapter\"\n\n        prepared_data.append({\n            'input': input_text,\n            'output': answer,\n            'adapter': adapter\n        })\n\n    return prepared_data\n\n# Prepare data for training set\ntrain_data = prepare_data_for_model(dataset['train'])\nprint(train_data[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:21:31.689706Z","iopub.execute_input":"2025-05-09T10:21:31.689939Z","iopub.status.idle":"2025-05-09T10:21:37.707670Z","shell.execute_reply.started":"2025-05-09T10:21:31.689915Z","shell.execute_reply":"2025-05-09T10:21:37.706748Z"}},"outputs":[{"name":"stdout","text":"{'input': \"Question: Who won the 1982 Illinois gubernatorial election, and how many votes was the margin? Context: 1982 Illinois gubernatorial election Table: [('Party', '-'), ('Party', 'Republican'), ('Candidate', 'James R. Thompson (incumbent)'), ('Votes', '1,816,101'), ('%', '49.44'), ('Â±', '-'), ('Party', '-'), ('Party', 'Democratic'), ('Candidate', 'Adlai Stevenson III'), ('Votes', '1,811,027'), ('%', '49.30'), ('Â±', '-'), ('Party', '-'), ('Party', 'Libertarian'), ('Candidate', 'Bea Armstrong'), ('Votes', '24,417'), ('%', '0.66'), ('Â±', '-'), ('Party', '-'), ('Party', 'Taxpayers'), ('Candidate', 'John E. Roche'), ('Votes', '22,001'), ('%', '0.60'), ('Â±', '-'), ('Party', '-'), ('Party', 'N/A'), ('Candidate', 'write-ins'), ('Votes', '161'), ('%', '0.00'), ('Â±', 'n-a'), ('Party', 'Majority'), ('Party', 'Majority'), ('Candidate', 'Majority'), ('Votes', '5,074'), ('%', '0.14'), ('Â±', '-'), ('Party', 'Turnout'), ('Party', 'Turnout'), ('Candidate', 'Turnout'), ('Votes', '3,673,707'), ('%', '-'), ('Â±', '-'), ('Party', '-'), ('Party', 'Republican hold'), ('Candidate', 'Republican hold'), ('Votes', 'Swing'), ('%', '-'), ('Â±', '-')]\", 'output': 'Thompson prevailed in the 1982 Illinois gubernatorial election by a 5,074 vote margin.', 'adapter': 'tabular_adapter'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:21:37.708577Z","iopub.execute_input":"2025-05-09T10:21:37.708813Z","iopub.status.idle":"2025-05-09T10:21:40.783142Z","shell.execute_reply.started":"2025-05-09T10:21:37.708787Z","shell.execute_reply":"2025-05-09T10:21:40.782206Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom peft import get_peft_model, LoraConfig\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\n\n# Load pre-trained BART model and tokenizer\nmodel_name = 'facebook/bart-large'\ntokenizer = BartTokenizer.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\n# Define LoRA configuration for parameter-efficient fine-tuning\nlora_config = LoraConfig(\n    r=8,  # rank of the low-rank matrices\n    lora_alpha=32,  # scaling factor for LoRA weights\n    lora_dropout=0.1,  # dropout rate for LoRA layers\n    bias=\"none\",  # LoRA bias setting (none, all, or simple)\n    task_type=\"CAUSAL_LM\"  # Causal language modeling task type\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Freeze all layers except LoRA layers\nfor name, param in model.named_parameters():\n    if \"lora\" not in name:\n        param.requires_grad = False\n\n# Dataset class\nclass QADataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=1024):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        inputs = self.tokenizer(\n            item['input'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        labels = self.tokenizer(\n            item['output'],\n            max_length=128,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        # Mask padding tokens for the loss calculation\n        labels_ids = labels[\"input_ids\"].squeeze()\n        labels_ids[labels_ids == self.tokenizer.pad_token_id] = -100\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": labels_ids,\n        }\n\n# # Dummy data\n# train_data = [\n#     {'input': 'What is the capital of France?', 'output': 'Paris'},\n#     {'input': 'What is the largest planet in our solar system?', 'output': 'Jupiter'}\n# ]\n\n# Create DataLoader\ntrain_dataset = QADataset(train_data, tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n\n# Optimizer for LoRA parameters\noptimizer = AdamW(model.parameters(), lr=1e-4)\n\n# Gradient accumulation\naccumulation_steps = 4  # Accumulate gradients over 4 mini-batches\n\n# Move model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Adapter Layer Pruning Function\ndef prune_adapter_layers(model, layers_to_prune):\n    \"\"\"\n    Freeze (prune) specific adapter layers.\n    layers_to_prune: List of layer names to prune, e.g., [\"encoder.block.0.adapter\"]\n    \"\"\"\n    for name, param in model.named_parameters():\n        for layer_name in layers_to_prune:\n            if layer_name in name:\n                param.requires_grad = False\n                print(f\"Pruning layer: {name}\")\n\n# Example of pruning layers from the encoder\nlayers_to_prune = [\"encoder.block.0.adapter\", \"encoder.block.1.adapter\"]\nprune_adapter_layers(model, layers_to_prune)\n\n# Perform grid search over different layer combinations to prune\ndef grid_search_pruning(model, encoder_layer_ranges, decoder_layer_ranges):\n    for enc_layers in encoder_layer_ranges:\n        for dec_layers in decoder_layer_ranges:\n            # Construct layer names to prune based on the ranges provided\n            encoder_layers_to_prune = [f\"encoder.block.{i}.adapter\" for i in range(enc_layers[0], enc_layers[1] + 1)]\n            decoder_layers_to_prune = [f\"decoder.block.{i}.adapter\" for i in range(dec_layers[0], dec_layers[1] + 1)]\n            \n            # Combine encoder and decoder layers to prune\n            layers_to_prune = encoder_layers_to_prune + decoder_layers_to_prune\n            \n            # Prune the layers\n            print(f\"\\nPruning the following layers: {layers_to_prune}\")\n            prune_adapter_layers(model, layers_to_prune)\n            \n            # Perform your evaluation after pruning the layers (e.g., train and evaluate)\n            # Here, you can add code to evaluate the model's performance after pruning the layers\n            # For example: evaluate_model(model)\n\n# Define grid ranges for encoder and decoder layers to prune\nencoder_layer_ranges = [(0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11)]  # Prune from encoder layers 0 to 6, 0 to 7, etc.\ndecoder_layer_ranges = [(12, 18), (12, 19), (12, 20), (12, 21), (12, 22), (12, 23)]  # Prune from decoder layers 12 to 18, 12 to 19, etc.\n\n# Run grid search over layer pruning combinations\n# grid_search_pruning(model, encoder_layer_ranges, decoder_layer_ranges)\n\n# Training loop\nmodel.train()\nfor epoch in range(1,2):  # 1 epochs\n    print(f\"Epoch {epoch}\")\n    total_loss = 0\n    optimizer.zero_grad()\n\n    for step, batch in enumerate(tqdm(train_loader)):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n        # Loss calculation\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Backpropagation (gradient accumulation)\n        loss.backward()\n\n        # Perform optimizer step after 'accumulation_steps' mini-batches\n        if (step + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Average loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:21:40.784311Z","iopub.execute_input":"2025-05-09T10:21:40.784560Z","iopub.status.idle":"2025-05-09T11:11:43.958844Z","shell.execute_reply.started":"2025-05-09T10:21:40.784538Z","shell.execute_reply":"2025-05-09T11:11:43.958188Z"}},"outputs":[{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1832/1832 [50:00<00:00,  1.64s/it]","output_type":"stream"},{"name":"stdout","text":"Average loss: 1.6785\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model.save_pretrained(\"./fetaqa_bart_lora\")\ntokenizer.save_pretrained(\"./fetaqa_bart_lora\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:16:32.950594Z","iopub.execute_input":"2025-05-09T11:16:32.951270Z","iopub.status.idle":"2025-05-09T11:16:33.239404Z","shell.execute_reply.started":"2025-05-09T11:16:32.951246Z","shell.execute_reply":"2025-05-09T11:16:33.238808Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('./fetaqa_bart_lora/tokenizer_config.json',\n './fetaqa_bart_lora/special_tokens_map.json',\n './fetaqa_bart_lora/vocab.json',\n './fetaqa_bart_lora/merges.txt',\n './fetaqa_bart_lora/added_tokens.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, BartTokenizer\nfrom peft import PeftModel\n\ntokenizer = BartTokenizer.from_pretrained(\"./fetaqa_bart_lora\")\nbase_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\nmodel = PeftModel.from_pretrained(base_model, \"./fetaqa_bart_lora\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sacrebleu bert-score rouge_score datasets evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:42:03.949557Z","iopub.execute_input":"2025-05-09T11:42:03.949875Z","iopub.status.idle":"2025-05-09T11:42:09.290134Z","shell.execute_reply.started":"2025-05-09T11:42:03.949852Z","shell.execute_reply":"2025-05-09T11:42:09.289196Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\nRequirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.1.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=74e40ac2c5d06458f6e6973b9539c228f54d4997dae112ceaf33aa97ef738ce6\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import torch\nfrom evaluate import load as load_metric\nfrom tqdm import tqdm\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom bert_score import score as bertscore\nfrom sacrebleu import corpus_bleu\nfrom torch.utils.data import DataLoader\nimport os\n\n# Load validation data\nval_prepared = prepare_data_for_model(dataset[\"validation\"])\nval_dataset = QADataset(val_prepared, tokenizer)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n\n# Evaluation mode\nmodel.eval()\n\n# Prediction and reference containers\nall_preds = []\nall_refs = []\n\n# Run inference\nwith torch.no_grad():\n    for batch in tqdm(val_loader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=128,\n            num_beams=5,\n            early_stopping=True\n        )\n\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        # Fix label decoding (replace -100 with pad_token_id)\n        labels = batch[\"labels\"].clone()\n        labels[labels == -100] = tokenizer.pad_token_id\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n        all_preds.extend(decoded_preds)\n        all_refs.extend(decoded_labels)\n\n# ---------------------------\n# Evaluation Metrics\n# ---------------------------\n\n# SacreBLEU\nsacrebleu_score = corpus_bleu(all_preds, [all_refs])\nprint(f\"\\nðŸ”µ SacreBLEU Score: {sacrebleu_score.score:.2f}\")\n\n# ROUGE\nrouge = load_metric(\"rouge\")\nrouge_output = rouge.compute(predictions=all_preds, references=all_refs)\nprint(\"\\nðŸŸ¢ ROUGE Scores:\")\nfor key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n    score = rouge_output[key]\n    print(f\"{key.upper()}: F1={score:.4f}\")\n\n# BERTScore\nP, R, F1 = bertscore(all_preds, all_refs, lang=\"en\", rescale_with_baseline=True)\nprint(f\"\\nðŸ”´ BERTScore:\")\nprint(f\"Precision: {P.mean().item():.4f}\")\nprint(f\"Recall:    {R.mean().item():.4f}\")\nprint(f\"F1:        {F1.mean().item():.4f}\")\n\n# ---------------------------\n# Save Predictions and References\n# ---------------------------\n\nos.makedirs(\"eval_outputs\", exist_ok=True)\n\nwith open(\"eval_outputs/predictions_and_refs.txt\", \"w\", encoding=\"utf-8\") as f:\n    for pred, ref in zip(all_preds, all_refs):\n        f.write(f\"Prediction: {pred.strip()}\\n\")\n        f.write(f\"Reference:  {ref.strip()}\\n\")\n        f.write(\"-\" * 80 + \"\\n\")\n\nwith open(\"eval_outputs/predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n    for pred in all_preds:\n        f.write(pred.strip() + \"\\n\")\n\nwith open(\"eval_outputs/references.txt\", \"w\", encoding=\"utf-8\") as f:\n    for ref in all_refs:\n        f.write(ref.strip() + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:42:17.625223Z","iopub.execute_input":"2025-05-09T11:42:17.625535Z","iopub.status.idle":"2025-05-09T11:52:45.596404Z","shell.execute_reply.started":"2025-05-09T11:42:17.625509Z","shell.execute_reply":"2025-05-09T11:52:45.595426Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [10:26<00:00,  2.50s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ”µ SacreBLEU Score: 22.44\n\nðŸŸ¢ ROUGE Scores:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2338480277.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nðŸŸ¢ ROUGE Scores:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"rouge1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rouge2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rougeL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key}: P={score.precision:.4f}, R={score.recall:.4f}, F1={score.fmeasure:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'mid'"],"ename":"AttributeError","evalue":"'numpy.float64' object has no attribute 'mid'","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"import torch\nfrom evaluate import load as load_metric\nfrom tqdm import tqdm\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom bert_score import score as bertscore\nfrom sacrebleu import corpus_bleu\nfrom torch.utils.data import DataLoader\nimport os\n\n# ---------------------------\n# Evaluation Metrics\n# ---------------------------\n\n# SacreBLEU\nsacrebleu_score = corpus_bleu(all_preds, [all_refs])\nprint(f\"\\nðŸ”µ SacreBLEU Score: {sacrebleu_score.score:.2f}\")\n\n# ROUGE\nrouge = load_metric(\"rouge\")\nrouge_output = rouge.compute(predictions=all_preds, references=all_refs)\nprint(\"\\nðŸŸ¢ ROUGE Scores:\")\nfor key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n    score = rouge_output[key]\n    print(f\"{key.upper()}: F1={score:.4f}\")\n\n# BERTScore\nP, R, F1 = bertscore(all_preds, all_refs, lang=\"en\", rescale_with_baseline=True)\nprint(f\"\\nðŸ”´ BERTScore:\")\nprint(f\"Precision: {P.mean().item():.4f}\")\nprint(f\"Recall:    {R.mean().item():.4f}\")\nprint(f\"F1:        {F1.mean().item():.4f}\")\n\n# ---------------------------\n# Save Predictions and References\n# ---------------------------\n\nos.makedirs(\"eval_outputs\", exist_ok=True)\n\nwith open(\"eval_outputs/predictions_and_refs.txt\", \"w\", encoding=\"utf-8\") as f:\n    for pred, ref in zip(all_preds, all_refs):\n        f.write(f\"Prediction: {pred.strip()}\\n\")\n        f.write(f\"Reference:  {ref.strip()}\\n\")\n        f.write(\"-\" * 80 + \"\\n\")\n\nwith open(\"eval_outputs/predictions.txt\", \"w\", encoding=\"utf-8\") as f:\n    for pred in all_preds:\n        f.write(pred.strip() + \"\\n\")\n\nwith open(\"eval_outputs/references.txt\", \"w\", encoding=\"utf-8\") as f:\n    for ref in all_refs:\n        f.write(ref.strip() + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:55:45.290848Z","iopub.execute_input":"2025-05-09T11:55:45.291206Z","iopub.status.idle":"2025-05-09T11:56:13.728150Z","shell.execute_reply.started":"2025-05-09T11:55:45.291186Z","shell.execute_reply":"2025-05-09T11:56:13.727468Z"}},"outputs":[{"name":"stdout","text":"\nðŸ”µ SacreBLEU Score: 22.44\n\nðŸŸ¢ ROUGE Scores:\nROUGE1: F1=0.5623\nROUGE2: F1=0.3439\nROUGEL: F1=0.4666\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bcd599e7b8c4a3896971a2efe7eacf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"197a6503cf6d4f74b2759fb33dc2c6ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b63848e053432f9876507ca587ed50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce2e41ebcd743df8bd60ae25d1b9d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd697b1880b344d8a2ddd737b636fe99"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"134ec3715a7246cda7892313c621cb81"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nðŸ”´ BERTScore:\nPrecision: 0.5289\nRecall:    0.4606\nF1:        0.4935\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def generate_answer(question, table_text, model, tokenizer, device, max_input_len=512, max_output_len=128):\n    model.eval()\n    \n    input_text = f\"question: {question} table: {table_text}\"\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_input_len\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=max_output_len,\n            num_beams=5,\n            early_stopping=True\n        )\n\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\nquestion = \"What is the total sales in 2023?\"\ntable_text = \"Year | Sales\\n2022 | 200\\n2023 | 300\\n2024 | 250\"\n\nanswer = generate_answer(question, table_text, model, tokenizer, device)\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:03:01.254085Z","iopub.execute_input":"2025-05-09T12:03:01.254722Z","iopub.status.idle":"2025-05-09T12:03:01.778734Z","shell.execute_reply.started":"2025-05-09T12:03:01.254695Z","shell.execute_reply":"2025-05-09T12:03:01.778085Z"}},"outputs":[{"name":"stdout","text":"Answer: In 2023, the total sales are expected to reach 300,000.\n","output_type":"stream"}],"execution_count":23}]}